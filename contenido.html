<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Calidad de los Datos en la IA</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body {
      padding: 2rem;
      background-color: #f8f9fa;
    }
    h1 {
      text-align: center;
      margin-bottom: 2rem;
    }
    .accordion-body {
      text-align: justify;
    }
    .accordion-button {
      font-weight: 500;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Calidad de los Datos en la Inteligencia Artificial</h1>
    <div class="accordion" id="contenidoIA">

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading0">
          <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapse0" aria-expanded="true" aria-controls="collapse0">
            ¿Qué es la inteligencia artificial y por qué depende de los datos?
          </button>
        </h2>
        <div id="collapse0" class="accordion-collapse collapse show" aria-labelledby="heading0" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            La inteligencia artificial (IA) se puede definir como “la simulación realizada por máquinas o sistemas informáticos de procesos o de actividades realizadas por la inteligencia humana” (Bazán-Gil, 2020). Su efectividad depende directamente de los datos que procesa: los modelos de aprendizaje automático necesitan conjuntos de datos representativos y de calidad para poder entrenarse adecuadamente y generar predicciones útiles. Si los datos son erróneos, incompletos o inconsistentes, los modelos pueden producir resultados sesgados o inexactos, afectando su desempeño en áreas críticas como la salud, la educación y los servicios públicos.
          </div>
        </div>
      </div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading00">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse00" aria-expanded="false" aria-controls="collapse00">
            ¿Qué se entiende por calidad de datos?
          </button>
        </h2>
        <div id="collapse00" class="accordion-collapse collapse" aria-labelledby="heading00" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            La calidad de los datos se refiere al grado en que los datos son precisos, completos, consistentes y actualizados, y están disponibles en el momento adecuado para su uso. Esto implica no solo que los datos reflejen la realidad, sino también que estén organizados de forma coherente para cumplir con su propósito. Según Carrillo (2024), “la calidad de los datos es crucial para que las soluciones de inteligencia artificial respondan a las necesidades y al contexto para el cual han sido creadas” (p. 159).
          </div>
        </div>
      </div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading1">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse1" aria-expanded="false" aria-controls="collapse1">
            Tipos de errores y problemas comunes en los datos
          </button>
        </h2>
        <div id="collapse1" class="accordion-collapse collapse" aria-labelledby="heading1" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            Aunque aún no encontramos en las fuentes detalles específicos sobre los tipos de errores, podemos integrar los que suelen destacarse en la literatura técnica:<br>
            ● Ausencia de datos: valores faltantes que limitan el análisis y sesgan los resultados.<br>
            ● Inconsistencias: diferencias entre formatos o valores (por ejemplo, mediciones contradictorias).<br>
            ● Duplicaciones: registros repetidos que distorsionan estadísticas.<br>
            ● Valores atípicos: datos extremos sin relación con la distribución habitual, resultado de errores o situaciones excepcionales.
          </div>
        </div>
      </div>

        <div class="accordion-item">
        <h2 class="accordion-header" id="heading2">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse2" aria-expanded="false" aria-controls="collapse2">
            Importancia de mejorar la calidad de los datos
          </button>
        </h2>
        <div id="collapse2" class="accordion-collapse collapse" aria-labelledby="heading2" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            La falta de calidad de los datos puede generar sesgos en los modelos, producir resultados erróneos, aumentar la incertidumbre en las decisiones y afectar la confianza que los usuarios depositan en los sistemas de IA. Por ejemplo, un sistema de diagnóstico médico basado en datos incompletos o imprecisos podría generar un falso negativo, poniendo en riesgo la salud del paciente. Al mismo tiempo, en los negocios, datos financieros erróneos pueden resultar en pérdidas económicas significativas. Además, el coste de corregir problemas detectados tardíamente —durante o después de que el modelo ya esté en uso— suele ser mucho mayor que invertir en etapas tempranas de limpieza y validación. Sumado a esto, en la educación, sistemas automatizados pueden ofrecer recomendaciones inadecuadas si los datos estudiantiles están desactualizados o contienen errores, afectando el aprendizaje de los estudiantes.<br>
            La falta de calidad en los datos también puede provocar pérdida de confianza en la tecnología, aumentar los costos operativos por la necesidad de corregir errores y generar riesgos legales o éticos, especialmente si se toman decisiones discriminatorias o injustas (Pérez & Martínez, 2019).
          </div>
        </div>
      </div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading3">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse3" aria-expanded="false" aria-controls="collapse3">
            Fundamentos de la calidad de datos
          </button>
        </h2>
        <div id="collapse3" class="accordion-collapse collapse" aria-labelledby="heading3" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            Diversos autores y normativas, como ISO 25012, reconocen que la calidad de los datos puede analizarse a través de varias dimensiones clave.<br>
            <strong>1. Precisión.</strong> <br>La precisión se refiere a qué tan bien los datos reflejan la realidad. Si los valores son incorrectos o estimaciones imprecisas, las decisiones basadas en ellos estarán equivocadas. Según Redalyc (2024), este atributo es fundamental en modelos de IA para evitar predicciones erróneas.<br>
            <strong>2. Completitud.</strong> <br>La completitud implica que todos los datos necesarios para un análisis estén disponibles. Por ejemplo, un modelo predictivo de abandono escolar necesita información suficiente y diversa para detectar patrones válidos (Surutusa, 2024).<br>
            <strong>3. Consistencia.</strong> <br>La consistencia se refiere a que los datos no tengan contradicciones entre sí ni errores de formato. Carrillo (2024) afirma que una estructura de datos sólida es parte del gobierno de la información que permite garantizar la calidad en sistemas inteligentes.<br>
            <strong>4. Actualidad.</strong> <br>Los datos deben estar actualizados para reflejar las condiciones reales en el momento del análisis. La IA pierde valor si trabaja con datos obsoletos, ya que sus resultados no serán relevantes para el contexto actual.
          </div>
        </div>
      </div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading4">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse4" aria-expanded="false" aria-controls="collapse4">
            Consecuencias de una baja calidad de datos
          </button>
        </h2>
        <div id="collapse4" class="accordion-collapse collapse" aria-labelledby="heading4" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            La falta de calidad de los datos puede generar sesgos en los modelos, producir resultados erróneos, aumentar la incertidumbre en las decisiones y afectar la confianza que los usuarios depositan en los sistemas de IA. Por ejemplo, un sistema de diagnóstico médico basado en datos incompletos o imprecisos podría generar un falso negativo, poniendo en riesgo la salud del paciente. Al mismo tiempo, en los negocios, datos financieros erróneos pueden resultar en pérdidas económicas significativas. Además, el coste de corregir problemas detectados tardíamente —durante o después de que el modelo ya esté en uso— suele ser mucho mayor que invertir en etapas tempranas de limpieza y validación. Sumado a esto, en la educación, sistemas automatizados pueden ofrecer recomendaciones inadecuadas si los datos estudiantiles están desactualizados o contienen errores, afectando el aprendizaje de los estudiantes.<br>
            La falta de calidad en los datos también puede provocar pérdida de confianza en la tecnología, aumentar los costos operativos por la necesidad de corregir errores y generar riesgos legales o éticos, especialmente si se toman decisiones discriminatorias o injustas (Pérez & Martínez, 2019).          </div>
        </div>
      </div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading5">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse5" aria-expanded="false" aria-controls="collapse5">
            Ejemplos documentados en sectores como salud, finanzas y educación
          </button>
        </h2>
        <div id="collapse5" class="accordion-collapse collapse" aria-labelledby="heading5" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            <strong>Salud:</strong> Un modelo de predicción de enfermedades crónicas puede fallar si los historiales médicos contienen valores faltantes o mediciones inconsistentes. Estos errores dificultan el diagnóstico temprano y pueden derivar en tratamientos inadecuados.<br>
            <strong>Finanzas:</strong> Existen casos donde los algoritmos de evaluación crediticia han replicado sesgos históricos, negando préstamos a personas de ciertos perfiles debido a datos incompletos o mal clasificados (Pérez & Martínez, 2019).<br>
            <strong>Educación:</strong> Herramientas de evaluación predictiva sobre deserción escolar requieren información completa sobre rendimiento, asistencia y contexto socioeconómico. Datos incoherentes o faltantes pueden sesgar los modelos y empeorar las decisiones en políticas educativas.
          </div>
        </div>
      </div>

      <div class="accordion-item">
  <h2 class="accordion-header" id="heading6">
    <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse6" aria-expanded="false" aria-controls="collapse6">
      Técnicas estadísticas para mejorar la calidad de datos
    </button>
  </h2>
  <div id="collapse6" class="accordion-collapse collapse" aria-labelledby="heading6" data-bs-parent="#contenidoIA">
    <div class="accordion-body">
        Para enfrentar estos problemas, existen diversas técnicas estadísticas que ayudan a mejorar la calidad de los datos:
      <div class="accordion" id="subtecnicas">

        <div class="accordion-item">
          <h2 class="accordion-header" id="subheading1">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#subcollapse1" aria-expanded="false" aria-controls="subcollapse1">
              Limpieza de datos
            </button>
          </h2>
          <div id="subcollapse1" class="accordion-collapse collapse" aria-labelledby="subheading1" data-bs-parent="#subtecnicas">
            <div class="accordion-body">
              La limpieza de datos es el primer paso y consiste en identificar y corregir errores o inconsistencias en los conjuntos de datos. Algunas técnicas comunes son:<br><br>
              <strong>• Identificación de valores atípicos:</strong> Según AWS (s.f.) y OBS Business School (2025), los valores atípicos son datos que se alejan significativamente del resto, ya sea por errores en la entrada o por eventos excepcionales. Para detectarlos, se utilizan métodos estadísticos como el rango intercuartílico (IQR) o la puntuación Z (Z-score), los cuales permiten identificar qué datos están fuera de los límites normales. Una vez detectados, estos valores pueden eliminarse, corregirse o analizarse por separado según su relevancia para el estudio.<br>
              <strong>• Imputación de valores faltantes:</strong> Es común que algunos datos estén incompletos. Para no perder información valiosa, se reemplazan estos valores faltantes utilizando técnicas como la imputación por la media o mediana de la variable, o mediante modelos predictivos que estiman los valores basándose en otras variables relacionadas. También se pueden usar métodos más avanzados como la imputación por vecinos más cercanos (K-NN).<br>
              <strong>• Eliminación de duplicados:</strong> Los datos duplicados pueden distorsionar los resultados y aumentar el tamaño innecesariamente. Se identifican registros repetidos y se eliminan para asegurar que cada dato sea único y relevante (IBM, 2024).
            </div>
          </div>
        </div>

        <div class="accordion-item">
          <h2 class="accordion-header" id="subheading2">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#subcollapse2" aria-expanded="false" aria-controls="subcollapse2">
              Transformación de datos
            </button>
          </h2>
          <div id="subcollapse2" class="accordion-collapse collapse" aria-labelledby="subheading2" data-bs-parent="#subtecnicas">
            <div class="accordion-body">
              Una vez limpios, los datos suelen requerir transformación para que sean adecuados para los modelos de IA:<br><br>
              <strong>• Normalización y estandarización:</strong> Estas técnicas ajustan los datos para que todas las variables estén en una escala común. La normalización transforma los valores para que estén dentro de un rango específico (por ejemplo, entre 0 y 1), mientras que la estandarización los ajusta para que tengan media cero y desviación estándar uno. Como señalan la OBS Business School (2025) y DataScientest (s.f.), este tipo de transformación es esencial para algoritmos que dependen de la comparación entre distancias, como k-NN o los algoritmos de agrupamiento.<br>
              <strong>• Codificación de variables categóricas:</strong> Los modelos de IA trabajan con números, por lo que las variables que contienen texto o categorías deben convertirse en valores numéricos. Esto se hace mediante técnicas como el one-hot encoding, que crea columnas binarias para cada categoría, o el label encoding, que asigna números enteros a cada categoría.
            </div>
          </div>
        </div>

        <div class="accordion-item">
          <h2 class="accordion-header" id="subheading3">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#subcollapse3" aria-expanded="false" aria-controls="subcollapse3">
              Evaluación y validación estadística
            </button>
          </h2>
          <div id="subcollapse3" class="accordion-collapse collapse" aria-labelledby="subheading3" data-bs-parent="#subtecnicas">
            <div class="accordion-body">
              Después de limpiar y transformar los datos, es necesario evaluar su calidad para asegurarse de que los procesos hayan mejorado la información:<br><br>
              <strong>• Métricas de calidad:</strong> Se utilizan indicadores como la precisión, exactitud y recall para medir qué tan bien los datos procesados representan la realidad y permiten obtener resultados confiables.<br>
              <strong>• Pruebas estadísticas:</strong> Se aplican pruebas como t-test o ANOVA para comparar las distribuciones de los datos antes y después del procesamiento, verificando que las transformaciones no hayan introducido sesgos o errores adicionales.<br>
            </div>
          </div>
        </div>

      </div>
      <br>Estas técnicas forman parte de un proceso continuo que ayuda a mantener la calidad de los datos a lo largo del tiempo, asegurando que los modelos de inteligencia artificial sean cada vez más precisos y útiles en la práctica.
    </div>
  </div>
</div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading7">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse7" aria-expanded="false" aria-controls="collapse7">
            Herramientas tecnológicas para el procesamiento de datos
          </button>
        </h2>
        <div id="collapse7" class="accordion-collapse collapse" aria-labelledby="heading7" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            El procesamiento eficiente y confiable de datos es fundamental para garantizar la calidad necesaria en los modelos de inteligencia artificial. Para ello, existen múltiples herramientas tecnológicas que facilitan desde la limpieza y transformación hasta la validación y visualización de datos. Estas herramientas permiten optimizar el trabajo, reducir errores y acelerar el análisis, contribuyendo a obtener resultados más precisos y confiables.<br>
            <strong>Bibliotecas de Python: pandas, numpy, scikit-learn.</strong><br>
            ✓ Pandas: es una biblioteca fundamental para el análisis de datos estructurados. Facilita tareas como la lectura de archivos, limpieza, transformación y exploración de conjuntos de datos de gran tamaño de forma eficiente y con sintaxis accesible.<br>
            ✓ Numpy: proporciona soporte para operaciones matemáticas avanzadas y manipulación de arreglos multidimensionales. Esto permite realizar cálculos numéricos de manera rápida y precisa, y sirve como base para muchas otras bibliotecas del ecosistema de ciencia de datos en Python. Numpy es especialmente útil para operaciones vectorizadas que optimizan el procesamiento de grandes volúmenes de información.<br>
            ✓ Scikit-learn: aunque conocida principalmente como una biblioteca de aprendizaje automático, también ofrece una variedad de herramientas para el preprocesamiento de datos. Entre sus funciones más utilizadas están la normalización, imputación de valores faltantes y codificación de variables categóricas, lo que la convierte en un recurso clave en la etapa de preparación de datos.<br><br>
            <strong>Plataformas de código abierto para calidad de datos.</strong><br>
            Además de las bibliotecas de programación, existen plataformas especializadas que automatizan la validación y monitoreo de la calidad de datos.<br>
            Great Expectations es una plataforma de código abierto diseñada para definir, ejecutar y documentar pruebas de calidad de datos de manera automatizada. De acuerdo con RedALyC (2022), esta herramienta permite detectar de forma temprana problemas como valores fuera de rango, inconsistencias o datos faltantes, y presenta los resultados en reportes comprensibles para usuarios técnicos y no técnicos.<br>
            También existen otras plataformas open source con funcionalidades similares. Entre ellas se encuentran Deequ, desarrollada por Amazon, y Apache Griffin, orientada a garantizar la calidad de datos en entornos de big data. Como señala CONACYT (2021), estas herramientas permiten validar grandes volúmenes de información de forma escalable, facilitando la gobernanza de datos en contextos empresariales complejos; además de que son especialmente útiles para proyectos que requieren monitoreo continuo y escalable de la calidad de datos, integrándose fácilmente con pipelines de datos existentes.
            <br><br><strong>Ventajas de automatizar el preprocesamiento de datos.</strong><br>
            Automatizar el preprocesamiento de datos mediante herramientas especializadas aporta beneficios clave que fortalecen la calidad y confiabilidad del análisis. Uno de los principales es la eficiencia operativa, ya que se reduce la intervención manual en tareas repetitivas, acelerando los procesos de limpieza, transformación y validación de datos.<br>
            Asimismo, la automatización disminuye los errores humanos, al minimizar omisiones y fallos durante el manejo de información, lo cual incrementa la confiabilidad de los resultados. Otro aspecto relevante es la reproducibilidad, ya que los procesos estandarizados permiten obtener resultados consistentes cuando se aplican los mismos parámetros en diferentes ejecuciones.<br>
            Además, estas herramientas permiten establecer mecanismos de monitoreo continuo, lo que posibilita la detección oportuna de desviaciones o problemas en los datos conforme se actualizan, lo cual es fundamental en sistemas inteligentes que operan en tiempo real (RedALyC, 2022). Finalmente, al documentar y estandarizar los flujos de trabajo, la automatización facilita la colaboración interdisciplinaria, permitiendo que diversos miembros del equipo comprendan y reproduzcan fácilmente cada etapa del preprocesamiento (CONACYT, 2021).<br>
            En conjunto, estas ventajas contribuyen a fortalecer los cimientos sobre los cuales se construyen modelos de inteligencia artificial confiables, robustos y útiles para diversos entornos de aplicación.
          </div>
        </div>
      </div>

      <div class="accordion-item">
        <h2 class="accordion-header" id="heading8">
          <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapse8" aria-expanded="false" aria-controls="collapse8">
            Supervisión y mantenimiento de la calidad de datos
          </button>
        </h2>
        <div id="collapse8" class="accordion-collapse collapse" aria-labelledby="heading8" data-bs-parent="#contenidoIA">
          <div class="accordion-body">
            Mantener la calidad de los datos a lo largo del tiempo es un desafío constante en cualquier organización o proyecto que dependa de información para la toma de decisiones o para alimentar modelos de inteligencia artificial. La supervisión continua y el mantenimiento adecuado garantizan que los datos sigan siendo útiles, confiables y cumplan con estándares éticos y regulatorios.<br>
            <strong>Metodologías:</strong><br>
            Una de las metodologías más reconocidas para asegurar la calidad y gestión adecuada de los datos es la gobernanza de datos (Data Governance). Esta disciplina establece políticas, procesos y responsabilidades claras para garantizar la integridad, accesibilidad, seguridad y calidad de los datos dentro de una organización (Inesdi, 2025). La gobernanza de datos no solo se enfoca en la tecnología, sino también en roles específicos, como el data steward, que supervisa el correcto manejo de la información, asegurando que los datos sean confiables y estén bien documentados.<br>
            Por otro lado, los principios FAIR (Findable, Accessible, Interoperable, Reusable) constituyen un marco ampliamente reconocido para promover las buenas prácticas en la gestión, conservación y reutilización de datos científicos y tecnológicos. Tal como explican la Comisión Económica para América Latina y el Caribe (CEPAL, 2023) y RedALyC (2022), estos principios garantizan que los datos sean fácilmente localizables a través de metadatos precisos, accesibles mediante protocolos abiertos, interoperables con diferentes plataformas y reutilizables, gracias a la documentación clara sobre su procedencia, licencia y condiciones de uso.<br>
            La implementación de este enfoque no solo fomenta la transparencia y el intercambio de información entre comunidades científicas, sino que también facilita el monitoreo continuo y la conservación a largo plazo de los datos, aspectos esenciales para mantener su calidad a lo largo del tiempo y asegurar su utilidad en investigaciones futuras.<br>
            <br><strong>Uso de alertas y registros para identificar problemas futuros.</strong><br>
            El monitoreo efectivo de la calidad de los datos exige la implementación de sistemas capaces de detectar de forma temprana posibles anomalías o desviaciones en los conjuntos de datos. Para ello, se emplean mecanismos de alertas automáticas, que notifican a los usuarios cuando ciertos indicadores —como la tasa de valores faltantes o la aparición de datos fuera de rango— exceden umbrales previamente establecidos. De acuerdo con Nicolao (2023), este tipo de herramientas resulta fundamental para anticipar fallos en la integridad de los datos y evitar consecuencias negativas en procesos críticos.<br>
            Además, el uso de registros o logs permite documentar todas las modificaciones, eventos y accesos relacionados con los datos. Esta trazabilidad no solo facilita auditorías, sino que también ayuda a identificar patrones recurrentes que puedan comprometer la calidad de forma sistemática. En conjunto, estas estrategias permiten anticiparse a los problemas antes de que afecten directamente a los procesos operativos o modelos predictivos, favoreciendo una respuesta rápida y eficiente que reduce los riesgos y los costos asociados.<br><br>
            <strong>Retos comunes en el mantenimiento de la calidad de datos.</strong><br>
            A pesar de la existencia de metodologías consolidadas y herramientas avanzadas para el tratamiento y validación de datos, mantener la calidad de los datos sigue siendo un desafío considerable en muchos entornos organizacionales.<br>
            Uno de los principales obstáculos es el crecimiento exponencial del volumen y la variedad de los datos, lo que complica su estandarización y dificulta la implementación de controles consistentes entre diferentes fuentes (Inesdi, 2025). A esto se suma la fatiga del monitoreo, un fenómeno que ocurre cuando, con el tiempo, los equipos responsables disminuyen su nivel de atención o rigurosidad, generando omisiones que afectan la calidad general (Nicolao, 2023).<br>
            Los cambios tecnológicos y de procesos también representan un riesgo frecuente. Las actualizaciones en sistemas o modificaciones en los flujos de trabajo pueden introducir errores no detectados de forma inmediata, afectando la coherencia de los datos. A su vez, la falta de una cultura organizacional orientada a la gestión de datos constituye una barrera importante: sin el compromiso de todas las áreas, desde la dirección hasta los operadores, la gobernanza y el mantenimiento de la calidad se vuelven ineficaces (Inesdi, 2025).<br>
            Por otro lado, el cumplimiento de normativas y regulaciones, especialmente en lo relativo a privacidad y seguridad de la información, supone un reto constante debido a los cambios legislativos y a la necesidad de adaptar sistemas internos a nuevos estándares (CEPAL, 2023).<br>
            Superar estos desafíos implica más que tecnología: requiere una combinación de procesos claramente definidos, compromiso institucional y capacitación continua del personal, de manera que la calidad de los datos se mantenga como una prioridad estratégica en la organización.
          </div>
        </div>
      </div>

    </div>
  </div>
  <div class="text-center mt-5">
    <a href="index.html" class="btn btn-primary">Regresar al inicio</a>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>

